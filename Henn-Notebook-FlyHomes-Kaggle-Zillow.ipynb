{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FlyHomes Data Challenge: Kaggle Zillow Prize\n",
    "### Brian Henn - September 2018 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from scipy.stats import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define filename and paths\n",
    "fname_features_2016 = './data/properties_2016.csv'\n",
    "fname_features_2017 = './data/properties_2017.csv'\n",
    "fname_sales_2016 = './data/train_2016_v2.csv'\n",
    "fname_sales_2017 = './data/train_2017.csv'\n",
    "fname_sub = './data/sample_submission.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          logerror transactiondate\n",
      "parcelid                          \n",
      "11016594    0.0276      2016-01-01\n",
      "14366692   -0.1684      2016-01-01\n",
      "12098116   -0.0040      2016-01-01\n",
      "12643413    0.0218      2016-01-02\n",
      "14432541   -0.0050      2016-01-02\n"
     ]
    }
   ],
   "source": [
    "# load 2016 and 2017 sales data\n",
    "sales_2016 = pd.read_csv(fname_sales_2016, index_col=0, header=0, \n",
    "                         parse_dates=[2], infer_datetime_format=True)\n",
    "sales_2017 = pd.read_csv(fname_sales_2017, index_col=0, header=0, \n",
    "                         parse_dates=[2], infer_datetime_format=True)\n",
    "sales = pd.concat([sales_2016, sales_2017])\n",
    "sales_set = sales.index.unique().values\n",
    "print(sales.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brian/anaconda3/envs/insight/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2903: DtypeWarning: Columns (49) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if self.run_code(code, result):\n",
      "/home/brian/anaconda3/envs/insight/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2903: DtypeWarning: Columns (32,34) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if self.run_code(code, result):\n",
      "/home/brian/anaconda3/envs/insight/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2903: DtypeWarning: Columns (22,49) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if self.run_code(code, result):\n"
     ]
    }
   ],
   "source": [
    "# inner join 2016 and 2017 feature data with sales data on parcel id\n",
    "\n",
    "# first, load 2016 feature data, grabbing only the rows with sales data for memory purposes\n",
    "iter_csv = pd.read_csv(fname_features_2016, index_col=0, header=0, iterator=True, chunksize=50000)\n",
    "features_2016 = pd.concat([chunk[chunk.index.isin(sales_set)] for chunk in iter_csv])\n",
    "\n",
    "# now, join the feature data to the sales data, duplicating properties with multiple sales\n",
    "features_2016_with_sales = features_2016.merge(sales, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frac_at_least_one_missing = sum(features_2016_with_sales.isnull().sum(1) > 0)/len(features_2016_with_sales)\n",
    "print('Fraction of rows with at least one missing value: %0.4f.\\n' % (frac_at_least_one_missing))\n",
    "\n",
    "frac_missing = []\n",
    "for col in features_2016_with_sales.columns.values:\n",
    "    frac_missing.append((len(features_2016_with_sales[col]) - features_2016_with_sales[col].count())/len(\n",
    "        features_2016_with_sales[col]))\n",
    "    \n",
    "fig = plt.figure()\n",
    "fig.set_size_inches([8,15])\n",
    "ax = fig.subplots()\n",
    "ax.barh(range(len(frac_missing)), frac_missing)\n",
    "ax.set_xlim([0,1]) \n",
    "ax.set_xlabel('Fraction Missing', fontsize=16)\n",
    "ax.set_ylim([0,len(frac_missing)])\n",
    "ax.set_xticks(np.arange(0,1.01,0.1))\n",
    "ax.set_yticks(range(len(frac_missing)))\n",
    "ax.set_yticklabels(features_2016_with_sales.columns.values,rotation = 0);\n",
    "ax.grid(True,'major','y')\n",
    "ax.set_title('Fraction of Missing Data', fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce features to those with at least 75% of data\n",
    "\n",
    "features_2016_with_sales = features_2016_with_sales.iloc[:,[frac < 0.25 for frac in frac_missing]]\n",
    "frac_missing = [frac_missing[i] for i, _ in enumerate(frac_missing) if frac_missing[i] < 0.25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(features_2016_with_sales.shape)\n",
    "print(features_2016_with_sales.columns)\n",
    "print(features_2016_with_sales['propertylandusetypeid'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_2016_with_sales.iloc[:,:20].sample(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform categorical variable about property type into more useful set of dummy variables\n",
    "\n",
    "# single family homes (most of the dataset)\n",
    "features_2016_with_sales['single_family'] = [\n",
    "    1 if val == 261.0 else 0 for val in features_2016_with_sales['propertylandusetypeid']]\n",
    "# multi-family (duplex/triplex etc.)\n",
    "features_2016_with_sales['multi_family'] = [\n",
    "    1 if (val >= 246.0 and val <= 248.0) else 0 for val in features_2016_with_sales['propertylandusetypeid']]\n",
    "# condos (a lot of these also)\n",
    "features_2016_with_sales['condominium'] = [\n",
    "    1 if val == 266.0 else 0 for val in features_2016_with_sales['propertylandusetypeid']]\n",
    "# planned/cluster/mobile (a few of these)\n",
    "features_2016_with_sales['planned_community'] = [\n",
    "    1 if (val == 263.0 or val == 265.0 or val == 269.0) \\\n",
    "    else 0 for val in features_2016_with_sales['propertylandusetypeid']]\n",
    "# everything else (small number of random unusual residential types and commercial properties)\n",
    "features_2016_with_sales['other_property'] = [\n",
    "    1 if val not in [246.0, 247.0, 248.0, 261.0, 263.0, 265.0, 266.0, \n",
    "        269.0] else 0 for val in features_2016_with_sales['propertylandusetypeid']]\n",
    "\n",
    "#features_2016_with_sales[['propertylandusetypeid','single_family','multi_family','condominium']].sample(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list useful features for inclusion in the model\n",
    "\n",
    "features_in_model = ['bedroomcnt','bathroomcnt','calculatedfinishedsquarefeet','fullbathcnt',\n",
    "                     'latitude','longitude','lotsizesquarefeet','single_family','multi_family','condominium',\n",
    "                    'planned_community','other_property','regionidzip','yearbuilt','taxvaluedollarcnt']\n",
    "\n",
    "X = features_2016_with_sales[features_in_model]\n",
    "X.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute missing values using sklearn's implementation\n",
    "\n",
    "# impute using the mean as a simple solution\n",
    "imputer = Imputer(strategy='mean')\n",
    "X_imputed = X.copy()\n",
    "for col in X.columns.values:\n",
    "    if sum(X[col].isna()) > 0: # only apply this to columns with missing data\n",
    "        imputer.fit(X[[col]])\n",
    "        X_imputed[col] = imputer.transform(X[[col]]).ravel()\n",
    "\n",
    "X_imputed.sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get sales data as training dataset\n",
    "\n",
    "Y = features_2016_with_sales[['logerror','transactiondate']]\n",
    "Y.sample(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine the correlations among the selected features and with the log error\n",
    "\n",
    "# https://seaborn.pydata.org/examples/many_pairwise_correlations.html\n",
    "\n",
    "# set up colormap for correlation plots\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "cmax = 1\n",
    "cmin = -1\n",
    "\n",
    "# create a dataframe of just the features we want to look at, plus the target)\n",
    "plot_columns = features_in_model\n",
    "plot_columns.append('logerror')\n",
    "full_df = features_2016_with_sales[plot_columns]\n",
    "                                                    \n",
    "# Compute the correlation matrix\n",
    "corr_properties = full_df.corr()\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.zeros_like(corr_properties, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(11, 9))\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(corr_properties, mask=mask, cmap=cmap, vmax=cmax, vmin=cmin, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now bar plot just correlation of logerror against features\n",
    "\n",
    "print(corr_properties.shape)\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.set_size_inches([8,6])\n",
    "ax = fig.subplots()\n",
    "ax.bar(range(len(full_df.columns.values) - 1), corr_properties.iloc[-1,0:15])\n",
    "ax.set_ylim([-0.25,0.25])\n",
    "ax.set_ylabel('Correlation with Log Error', fontsize=16)\n",
    "ax.set_xticks(range(len(full_df.columns.values) - 1))\n",
    "ax.set_xticklabels(full_df.columns.values[0:-1],rotation=-90, fontsize=16)\n",
    "ax.grid(True,'major','y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split sample into training and test sets \n",
    "# retain 20% of the properties with sales (34k) as a test dataset, train on 80% (134k)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_imputed, Y['logerror'], test_size=0.2, shuffle=True)\n",
    "\n",
    "# Note: for the moment we are not worry about the time dimension of the sales,\n",
    "# instead just using all of the sales from 2016 and 2017 as both training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's train the gradient boosted tree model's hyperparameters using sklearn's RandomSearchCV capabilities\n",
    "\n",
    "# define the estimator as GBT\n",
    "gbt = GradientBoostingRegressor(loss='lad')\n",
    "\n",
    "# define the hyperparameter we want to search across in random sampling (as in section 2)\n",
    "param_dist = {'n_estimators' : randint(2, 50), # number of boosting iterations \n",
    "              \"max_depth\": randint(3, 10), # allow for potentially many (100 splits) in the trees\n",
    "              \"max_features\": [None, 'sqrt'], # allow either any number of features in bagging, or just the sqrt\n",
    "              \"min_samples_split\": randint(2, 11), # controls on splitting and leaves\n",
    "              \"min_samples_leaf\": randint(1, 11),\n",
    "              \"criterion\": ['friedman_mse', 'rmse', 'mae']} # vary the scoring method\n",
    "# random sampling strategy suggested by scikit-learn.org/stable/auto_examples/model_selection/plot_randomized_search.html#sphx-glr-auto-examples-model-selection-plot-randomized-search-py\n",
    "\n",
    "# randomly search parameters using my laptop's cores\n",
    "n_iter_random = 20 # 5 iterations of parameters for each fold \n",
    "cv_random = 5 # 5-fold cross validation\n",
    "gbt_hypertuning = RandomizedSearchCV(estimator=gbt, param_distributions=param_dist, n_iter=n_iter_random,\n",
    "                               cv=cv_random, random_state=91214, n_jobs=-1, verbose=4, return_train_score=True)\n",
    "\n",
    "# Fit the random search model for hyperparameter tuning\n",
    "gbt_hypertuning.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(gbt_hypertuning.cv_results_)\n",
    "results_df.sort_values('mean_test_score', ascending=False, inplace=True)\n",
    "results_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model using best hyperparameters and whole training dataset\n",
    "\n",
    "# use the best hyperparameters\n",
    "best = gbt_hypertuning.best_params_\n",
    "print(best)\n",
    "gbt_train = GradientBoostingRegressor(**best)\n",
    "\n",
    "# train the model\n",
    "gbt_train.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate best model on test dataset\n",
    "\n",
    "# produce predictions \n",
    "Y_pred = gbt_train.predict(X_test)\n",
    "\n",
    "# compute MAE on predictions\n",
    "MAE = mean_absolute_error(Y_pred, Y_test)\n",
    "print(MAE)\n",
    "\n",
    "gbt_train.score(X_test, Y_test, sample_weight=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(Y_test, Y_pred, 'b +')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:insight]",
   "language": "python",
   "name": "conda-env-insight-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
